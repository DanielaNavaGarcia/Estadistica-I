{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJD5v0hqDovwcQ94o82KEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielaNavaGarcia/Estadistica-I/blob/main/Programas_Optimizaci%C3%B3n_sin_restricciones_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fletcher Reeves"
      ],
      "metadata": {
        "id": "BCRlPPJADsAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWI5nrWRDnKN",
        "outputId": "40939baa-e68b-4e0e-9341-7f21242a01ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M√≠nimo encontrado en: [0. 0.]\n",
            "Valor m√≠nimo de f: 13.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize_scalar\n",
        "\n",
        "def fletcher_reeves(f, grad_f, x0, tol=1e-6, max_iter=1000):\n",
        "    \"\"\"\n",
        "    M√©todo de Fletcher-Reeves para optimizaci√≥n sin restricciones.\n",
        "\n",
        "    Par√°metros:\n",
        "    - f: funci√≥n objetivo, f(x)\n",
        "    - grad_f: gradiente de f, ‚àáf(x)\n",
        "    - x0: punto inicial (numpy array)\n",
        "    - tol: tolerancia para el criterio de parada\n",
        "    - max_iter: n√∫mero m√°ximo de iteraciones\n",
        "\n",
        "    Retorna:\n",
        "    - x: punto que minimiza f(x)\n",
        "    - history: lista de puntos visitados\n",
        "    \"\"\"\n",
        "    x = x0\n",
        "    g = grad_f(x)\n",
        "    d = -g\n",
        "    history = [x.copy()]\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        # B√∫squeda lineal: minimizar f(x + alpha*d)\n",
        "        phi = lambda alpha: f(x + alpha * d)\n",
        "        res = minimize_scalar(phi)\n",
        "        alpha = res.x\n",
        "\n",
        "        # Actualizaci√≥n del punto\n",
        "        x_new = x + alpha * d\n",
        "        g_new = grad_f(x_new)\n",
        "\n",
        "        # Criterio de convergencia\n",
        "        if np.linalg.norm(g_new) < tol:\n",
        "            history.append(x_new.copy())\n",
        "            break\n",
        "\n",
        "        # C√°lculo de beta (Fletcher-Reeves)\n",
        "        beta = np.dot(g_new, g_new) / np.dot(g, g)\n",
        "\n",
        "        # Actualizaci√≥n de direcci√≥n y gradiente\n",
        "        d = -g_new + beta * d\n",
        "        g = g_new\n",
        "        x = x_new\n",
        "        history.append(x.copy())\n",
        "\n",
        "    return x, history\n",
        "\n",
        "# =======================\n",
        "# üåü Ejemplo de uso\n",
        "# =======================\n",
        "\n",
        "# Funci√≥n objetivo: f(x) = (x1 - 2)^2 + (x2 + 3)^2\n",
        "def f(x):\n",
        "    return (x[0] - 2)**2 + (x[1] + 3)**2\n",
        "\n",
        "# Gradiente de f\n",
        "def grad_f(x):\n",
        "    return np.array([2*(x[0] - 2), 2*(x[1] + 3)])\n",
        "\n",
        "# Punto inicial\n",
        "x0 = np.array([0.0, 0.0])\n",
        "\n",
        "# Llamada al m√©todo\n",
        "xmin, hist = fletcher_reeves(f, grad_f, x0)\n",
        "\n",
        "print(\"M√≠nimo encontrado en:\", xmin)\n",
        "print(\"Valor m√≠nimo de f:\", f(xmin))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "M√©todo de Newton"
      ],
      "metadata": {
        "id": "mqpg_LkkDzp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def newton_method(f, grad_f, hess_f, x0, tol=1e-6, max_iter=100):\n",
        "    \"\"\"\n",
        "    M√©todo de Newton para optimizaci√≥n sin restricciones.\n",
        "\n",
        "    Par√°metros:\n",
        "    - f: funci√≥n objetivo\n",
        "    - grad_f: funci√≥n que retorna el gradiente (‚àáf)\n",
        "    - hess_f: funci√≥n que retorna la Hessiana (Hf)\n",
        "    - x0: punto inicial (numpy array)\n",
        "    - tol: tolerancia para el gradiente\n",
        "    - max_iter: n√∫mero m√°ximo de iteraciones\n",
        "\n",
        "    Retorna:\n",
        "    - x: punto que minimiza f(x)\n",
        "    - history: lista de puntos visitados\n",
        "    \"\"\"\n",
        "    x = x0\n",
        "    history = [x.copy()]\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        g = grad_f(x)\n",
        "        H = hess_f(x)\n",
        "\n",
        "        if np.linalg.norm(g) < tol:\n",
        "            break\n",
        "\n",
        "        # Resolver el sistema H p = -g\n",
        "        try:\n",
        "            p = np.linalg.solve(H, -g)\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(\"Hessiana no invertible.\")\n",
        "            break\n",
        "\n",
        "        x = x + p\n",
        "        history.append(x.copy())\n",
        "\n",
        "    return x, history\n",
        "\n",
        "# =======================\n",
        "# üåü Ejemplo de uso\n",
        "# =======================\n",
        "\n",
        "# f(x, y) = (x - 2)^2 + (y + 3)^2\n",
        "def f(x):\n",
        "    return (x[0] - 2)**2 + (x[1] + 3)**2\n",
        "\n",
        "def grad_f(x):\n",
        "    return np.array([2*(x[0] - 2), 2*(x[1] + 3)])\n",
        "\n",
        "def hess_f(x):\n",
        "    return np.array([[2, 0], [0, 2]])\n",
        "\n",
        "# Punto inicial\n",
        "x0 = np.array([0.0, 0.0])\n",
        "\n",
        "# Llamada al m√©todo\n",
        "xmin, hist = newton_method(f, grad_f, hess_f, x0)\n",
        "\n",
        "print(\"M√≠nimo encontrado en:\", xmin)\n",
        "print(\"Valor m√≠nimo de f:\", f(xmin))\n"
      ],
      "metadata": {
        "id": "34vuJyGFD3aA",
        "outputId": "67155183-0204-47a1-f0d2-6a13c6670d45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M√≠nimo encontrado en: [ 2. -3.]\n",
            "Valor m√≠nimo de f: 0.0\n"
          ]
        }
      ]
    }
  ]
}